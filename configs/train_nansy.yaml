pl:
  checkpoint:
    callback:
      save_top_k: -1
      monitor: "g/train_L1"
      verbose: True
      every_n_epochs: 5 #epochs

  trainer:
    gradient_clip_val: 0 # don't clip (default value)
    max_epochs: 10000
    num_sanity_val_steps: 1
    fast_dev_run: False
    check_val_every_n_epoch: 1
    progress_bar_refresh_rate: 1
    accelerator: "ddp"
    benchmark: True

logging:
  log_dir: /raid/vision/dhchoi/log/nansy/
  seed: "04_00"

  save_files: [
      './*.py',
      './*.sh',
      'configs/*.*',
      'datasets/*.*',
      'models/*.*',
      'utils/*.*',
  ]

datasets:
  train:
    class: datasets.base.MultiDataset
    datasets: [
        'configs/datasets/vctk.yaml',
    ]

    mode: train
    batch_size: 2
    shuffle: True
    num_workers: 32

  eval:
    class: datasets.base.MultiDataset
    datasets: [
        'configs/datasets/vctk.yaml',
    ]

    mode: eval
    batch_size: 2
    shuffle: False
    num_workers: 4


models:
  Analysis:
    class: models.analysis.Analysis

    optim:
      class: torch.optim.Adam
      kwargs:
        lr: 1e-4
        betas: [ 0.5, 0.9 ]

  Synthesis:
    class: models.synthesis.Synthesis

    optim:
      class: torch.optim.Adam
      kwargs:
        lr: 1e-4
        betas: [ 0.5, 0.9 ]

#  Discriminator:
#    class: models.synthesis.Discriminator
#
#    optim:
#      class: torch.optim.Adam
#     kwargs:
#        lr: 1e-4
#        betas: [ 0.5, 0.9 ]
